{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "semester_project_colab.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kJ7PGDjKwDE"
      },
      "source": [
        "#ME536 - Semester Project\r\n",
        "Ä°smail Melih CANBOLAT - 2033199"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtrC9m1vVg8N"
      },
      "source": [
        "#Imports\r\n",
        "Required packages are imported."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKJTtFYWU3Et"
      },
      "source": [
        "import numpy as np\r\n",
        "import cv2 as cv\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "%matplotlib inline\r\n",
        "\r\n",
        "from tensorflow.keras.applications.vgg19 import VGG19\r\n",
        "from tensorflow.keras.preprocessing import image\r\n",
        "from tensorflow.keras.applications.vgg19 import preprocess_input\r\n",
        "from tensorflow.keras.models import Model\r\n",
        "\r\n",
        "from sklearn.decomposition import PCA\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eKS9Yt0QdWa"
      },
      "source": [
        "# # Download sample images from GitHub - For test purposes\r\n",
        "# import os\r\n",
        "# !rm *.jpg  # Remove previous images\r\n",
        "# img_count = 9 # Number of images available at the link\r\n",
        "# for i in range(img_count):\r\n",
        "#     os.system(\"wget https://raw.githubusercontent.com/melih-canbolat/\"+\r\n",
        "#               \"ME536-Project/main/Webots-Simulation/controllers/slave/\"+\r\n",
        "#               \"captures/img\"+str(i)+\".jpg\")\r\n",
        "#     print(\"img\"+str(i)+\".jpg\")\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xraK9JavYYfN"
      },
      "source": [
        "def show(im):\r\n",
        "    \"\"\"\r\n",
        "    Simple function to display an image\r\n",
        "    \"\"\"\r\n",
        "    plt.imshow(im, cmap = 'gray')\r\n",
        "    plt.show()\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iK9HF11ZiHk5"
      },
      "source": [
        "class Item:\r\n",
        "    \"\"\"\r\n",
        "    This class is used to create an instance for each detected item/object on the image.\r\n",
        "    \"\"\"\r\n",
        "    def __init__(self, stats, image):\r\n",
        "        self.x = stats[0]       # leftmost (x) coordinate\r\n",
        "        self.y = stats[1]       # topmost (y) coordinate\r\n",
        "        self.width = stats[2]   # horizontal size of the bounding box\r\n",
        "        self.height = stats[3]  # vertical size of the bounding box\r\n",
        "        self.frame(image)       # Extract the Region of Interest from the image\r\n",
        "\r\n",
        "    def frame(self, src):\r\n",
        "        i = src[self.y:self.y+self.height+1, self.x:self.x+self.width+1]  # Extract the ROI from orginal capture\r\n",
        "        self.img = cv.cvtColor(i, cv.COLOR_BGR2RGB)  # RGB version of the original image of the item\r\n",
        "        \r\n",
        "        # === UNUSED PART - REMOVE === #\r\n",
        "        # Upscale the image with certain interpolation algorithm to obtain smoother & larger image\r\n",
        "        # self.resized = cv.resize(img,None,fx=3, fy=3, interpolation = cv.INTER_CUBIC)  # Slower\r\n",
        "        # img_resized = cv.resize(img,None,fx=3, fy=3, interpolation = cv.INTER_LINEAR)  # Faster\r\n",
        "        # img_resized = cv.resize(img,(224,224), interpolation = cv.INTER_LINEAR)  # Faster\r\n",
        "        # img_padded = cv.copyMakeBorder(img_resized,10,10,10,10,cv.BORDER_CONSTANT,value=0)  # Zero padding\r\n",
        "        # self.img_resized = img_resized\r\n",
        "        # self.img_padded = img_padded  # Padded image will be used for feature extraction.\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_1v8unoXXAU"
      },
      "source": [
        "detected = []  # List to store instances created for each detected item\r\n",
        "dataset = np.zeros((25088,1), dtype=\"float32\")  # Numpy array of feature vectors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IauoTCLrSNV0"
      },
      "source": [
        "#Image Processing\r\n",
        "Does following:\r\n",
        "\r\n",
        "*   Convert the image to grayscale\r\n",
        "*   Apply threshold to convert to binary image\r\n",
        "*   Invert binary image so that objects of interest are white\r\n",
        "*   Apply erosion followed by dilation to remove noise\r\n",
        "*   Use Connected Component Labeling algorithm to detect objects on the image\r\n",
        "*   Create an instance for each detected item & store them in a list\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cwW2QZGVgQQ"
      },
      "source": [
        "def process(image):\r\n",
        "    \"\"\"\r\n",
        "    Accepts the captured image and processes that image to detect objects \r\n",
        "    in it. Then, creates an instance Item class for each detected item and\r\n",
        "    stores them in the list: \"detected[]\"\r\n",
        "    \"\"\"\r\n",
        "    gimg = cv.cvtColor(image, cv.COLOR_BGR2GRAY)                                # Convert to grayscale\r\n",
        "    ret, thresh = cv.threshold(gimg, 0, 255, cv.THRESH_BINARY+cv.THRESH_OTSU)   # Apply OTSU threshold\r\n",
        "    th1 = np.invert(thresh)                                                     # Invert binary image so that objects of interest are white\r\n",
        "    kernel = np.ones((2,2),np.uint8)                                            # Define a kernel to be used for morphological transformations\r\n",
        "    img_final = cv.morphologyEx(th1, cv.MORPH_OPEN, kernel)                     # Erosion followed by Dilation - removes noise\r\n",
        "    # erosion = cv.erode(th,kernel,iterations = 1)    # Erosion only\r\n",
        "    # dilation = cv.dilate(th,kernel,iterations = 1)  # Dilation only\r\n",
        "    n, labels, stats, centroids = cv.connectedComponentsWithStats(img_final)    # Connected Component Labeling (with stats)\r\n",
        "\r\n",
        "    # Create an instance for each detected item (except background) & add it to the list\r\n",
        "    for i in range(1, n):\r\n",
        "        detected.append(Item(stats[i], image))\r\n",
        "\r\n",
        "\r\n",
        "    # === Draw bounding boxes - for debugging purposes === #\r\n",
        "    if 0:\r\n",
        "        img_boxed = cv.cvtColor(img_final, cv.COLOR_GRAY2BGR)\r\n",
        "        for i in detected:\r\n",
        "            cv.rectangle(img_boxed, (i.x, i.y), (i.x+i.width, i.y+i.height), (255,0,0), 1)\r\n",
        "        show(img_boxed)\r\n",
        "\r\n",
        "\r\n",
        "    # ==== Following can be considered for improvements ==== #\r\n",
        "    # img_blur = cv.blur(img,(10,10)) # Blur the image using a simple kernel of size 10x10\r\n",
        "    # contours, hierarchy = cv.findContours(thresh, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)  # Find the contours on a binary image\r\n",
        "    # cv.drawContours(img, contours, -1, (0, 255, 0), 3)  # Draw contours on img\r\n",
        "\r\n",
        "    return None\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKBIgtj-iO0w"
      },
      "source": [
        "# # === TEST SECTION === #\r\n",
        "# captured = cv.imread(\"img0.jpg\")  # Read an image (BGR)\r\n",
        "# x = process(captured)  # Proces that image and update the list: \"detected[]\"\r\n",
        "# print(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RUazBWXd3K0"
      },
      "source": [
        "#Feature Extraction Using **C**onvolutional **N**eural **N**etworks\r\n",
        "A model is created based on VGG19 so that a feature vector is extracted from an intermediate layer of the VGG19 network.  \r\n",
        "Original RGB image of the detected object is fed into the network. And for each object, feature vectors are obtained.  \r\n",
        "\r\n",
        "More info on VGG19:  \r\n",
        "https://keras.io/api/applications/vgg/#vgg19-function\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxQ8wBM9ZEit"
      },
      "source": [
        "First, create the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_vUb6z61hZI"
      },
      "source": [
        "base_model = VGG19(weights='imagenet')  # Base model is VGG19 with pre-trained weights\r\n",
        "\r\n",
        "# Do not use dense layers of VGG19. Get the output from \"flatten\" layer of VGG19.\r\n",
        "# i.e. Extract feature vector from an intermediate layer of VGG19\r\n",
        "model = Model(inputs=base_model.input, outputs=base_model.get_layer('flatten').output) \r\n",
        "\r\n",
        "# ===== From Example Code =====\r\n",
        "# img_path = 'img0.jpg'\r\n",
        "# img = image.load_img(img_path, target_size=(224, 224))\r\n",
        "# x = image.img_to_array(img)\r\n",
        "# x = np.expand_dims(x, axis=0)\r\n",
        "# x = preprocess_input(x)\r\n",
        "# feature_vector = model.predict(x)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DsYeIN6OyEy_"
      },
      "source": [
        "# model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnWVGvN8-yHs"
      },
      "source": [
        "# # ===== Prepare an image to extract its Feature Vector with the ANN Model =====\r\n",
        "# x = cv.cvtColor(detected[0].img_resized, cv.COLOR_GRAY2BGR)\r\n",
        "# x = x.astype('float32')\r\n",
        "# x = np.expand_dims(x, axis=0)\r\n",
        "# x = preprocess_input(x)\r\n",
        "\r\n",
        "# feature_vector = model.predict(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CY32sG11zsLB"
      },
      "source": [
        "def analyze(x):\r\n",
        "    \"\"\"\r\n",
        "    This function accepts an image(RGB) and returns feature vector for that image.\r\n",
        "    \"\"\"\r\n",
        "    x = cv.resize(x, (224, 224), interpolation = cv.INTER_LINEAR);  # Resize the image so that its dimesions are suitable for VGG19\r\n",
        "    x = x.astype('float32')  # Convert data type to float32\r\n",
        "    x = np.expand_dims(x, axis=0)  # Expand dimesnions\r\n",
        "    x = preprocess_input(x)  # Preproces the image for vgg19\r\n",
        "\r\n",
        "    feature_vector = model.predict(x)  # Get the feature vector from the CNN\r\n",
        "\r\n",
        "    return feature_vector\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4ulvhnCdqfs"
      },
      "source": [
        "captured = cv.imread(\"img8.jpg\")  # Read an image (BGR)\r\n",
        "process(captured)  # Proces that image and update the list: \"detected[]\"\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCnaMVEd_b-i"
      },
      "source": [
        "# Data Analysis - Incomplete\r\n",
        "Now that we have feature vectors, we can use them as data points in our dataset and determine if a newly added data is new or not.\r\n",
        "\r\n",
        "Steps:\r\n",
        "\r\n",
        "*   Dimension reduction of feature vectors using PCA\r\n",
        "*   Clustering data\r\n",
        "*   Detecting new cluster formation\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgZ7yqP2EPJJ"
      },
      "source": [
        "fv = analyze(detected[i].img)  # Analyze images in the detected list by feeding them into CNN and get the feature vector for them \r\n",
        "# dataset = np.append(dataset, np.transpose(fv), axis=1)  # Add the feature vector to dataset as a column vector\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jYV-vdkNWiE"
      },
      "source": [
        "pca = PCA(n_components=2)\r\n",
        "pca.fit(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NjF5GcxSrHd"
      },
      "source": [
        "u, s, vt = np.linalg.svd(dataset, full_matrices=True)\r\n",
        "plt.plot(s, \"k.\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}